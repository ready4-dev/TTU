---
title: "Reporting Workflow, AQoL-6D Clinical Population Example"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Reporting Workflow, AQoL-6D Clinical Population Example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

\blandscape
<!---BLOCK_LANDSCAPE_START--->

This vignette is an example of implementing of a reproducible workflow to develop transfer to AQoL-6D health utility models, create and share catalogues of these models and author a scientific summary of the results and methods deployed. This example, should be read in conjunction with [another similar workflow implemented for EQ-5D data and alternative predictors](CSP_EQ5D_TTU.html), which illustrates some additional features. The workflow illustrated in this vignette and its EQ-5D counterpart, is something that should only be undertaken once the optimal set of input parameters have been identified by an [initial exploratory analysis](Model_TTU.html). As with all the vignettes included with the TTU package, this example is illustrated using entirely fake data, so do not use its outputs to inform decision making. For an example of how TTU workflows were applied to real data see https://www.medrxiv.org/content/10.1101/2021.07.07.21260129v2.full .

```{r echo = TRUE, message=FALSE}
library(TTU)
```

# Input parameters

## Reporting parameters

### Bibliograhic data
We begin by specifying the authorship and other bibliographic data that we wish to attach to all reports that we generate.

```{r }
authors_tb <- ready4show::authors_tb
```
```{r authorsds, eval = knitr::is_html_output(), echo=F, results='asis'}
authors_tb %>%
  ready4show::print_table(output_type_1L_chr = "HTML",
                          caption_1L_chr = "Authors table",
                          mkdn_tbl_ref_1L_chr = "tab:authorsds")
```
```{r}
institutes_tb <- ready4show::institutes_tb
```
```{r institutesds, eval = knitr::is_html_output(), echo = F,results='asis'}
institutes_tb %>%
  ready4show::print_table(output_type_1L_chr = "HTML",
                          caption_1L_chr = "Author institutions table",
                          mkdn_tbl_ref_1L_chr = "tab:institutesds")
```

We combine the data about authors with other bibliographic information.

```{r}
header_yaml_args_ls <- make_header_yaml_args_ls(authors_tb = authors_tb,
                                                institutes_tb = institutes_tb,
                                                title_1L_chr = "A hypothetical study using fake data for instructional purposes only",
                                                keywords_chr = c("this","is","a","replication","using","fake","data","do", "not","cite"))
```

### Report formatting
We add parameters relating to the desired output format of the reports we will be producing.

```{r }
output_format_ls <- make_output_format_ls(manuscript_outp_1L_chr = "Word",
                                          manuscript_digits_1L_int = 2L,
                                          supplementary_outp_1L_chr = "PDF",
                                          supplementary_digits_1L_int = 2L)
```

## Data parameters

### Dataset
We read in our study dataset (in this case we are using fake data).

```{r message=FALSE}
ds_tb <- youthvars::replication_popl_tb %>% 
            youthvars::transform_raw_ds_for_analysis() 
```

The first few records of the dataset are as follows.

```{r inputds, echo=F, eval = knitr::is_html_output(), results='asis'}
ds_tb %>%
    head() %>%
  ready4show::print_table(output_type_1L_chr = "HTML",
                          caption_1L_chr = "Input dataset",
                          mkdn_tbl_ref_1L_chr = "tab:inputds")
```

### Data dictionary
A data dictionary must also be supplied for the dataset. The dictionary must be of class `ready4_dictionary` from the `ready4use` package.

```{r}
dictionary_tb <- youthvars::make_final_rpln_ds_dict()
```

The data dictionary is reproduced below.

```{r dictionary, echo=F, eval = knitr::is_html_output(), results='asis'}
dictionary_tb %>%
  ready4show::print_table(output_type_1L_chr = "HTML",
                          caption_1L_chr = "Data dictionary",
                          mkdn_tbl_ref_1L_chr = "tab:dictionary")
```


### Dataset metadata
We describe the features of our dataset that are most relevant to the analyses we are planning to undertake. 

```{r echo = TRUE}
ds_descvs_ls <- make_ds_descvs_ls(candidate_predrs_chr = c("K6","PHQ9"),#
                                  candidate_covar_nms_chr = c("d_age", "SOFAS",
                                                              "c_p_diag_s",
                                                              "c_clinical_staging_s", 
                                                              "d_relation_s",
                                                              "d_studying_working"),
                                  cohort_descv_var_nms_chr = c("d_age","d_relation_s",
                                                               "d_studying_working",
                                                               "c_p_diag_s","c_clinical_staging_s",
                                                               "SOFAS"),
                                  dictionary_tb = dictionary_tb, 
                                  id_var_nm_1L_chr = "fkClientID", 
                                  is_fake_1L_lgl = T,
                                  msrmnt_date_var_nm_1L_chr = "d_interview_date",
                                  round_var_nm_1L_chr = "round", 
                                  round_vals_chr = c("Baseline", "Follow-up"),
                                  maui_item_pfx_1L_chr = "aqol6d_q", 
                                  utl_wtd_var_nm_1L_chr = "aqol6d_total_w", 
                                  utl_unwtd_var_nm_1L_chr = "aqol6d_total_c")
```

### Candidate predictors metadata
We create a lookup table with metadata on the dataset variables that we specified in the `candidate_predrs_chr` element of `ds_descvs_ls` as our candidate predictors, as well as the preferred covariates that we will specify later in the `make_valid_params_ls_ls` function.

```{r}
predictors_lup <- make_pt_TTU_predictors_lup(short_name_chr = c(ds_descvs_ls$candidate_predrs_chr, "SOFAS"),
                                              long_name_chr = c("K6 total score", "PHQ9 total score", "SOFAS total score"),
                                              min_val_dbl = 0,
                                              max_val_dbl = c(24,27,100),
                                              class_chr = "integer",
                                              increment_dbl = 1,
                                              class_fn_chr = c("youthvars::youthvars_k6",
                                                               "youthvars::youthvars_phq9",
                                                               "youthvars::youthvars_sofas"),
                                              mdl_scaling_dbl = 0.01,
                                              covariate_lgl = F) %>%
  TTU_predictors_lup()
```

## Analysis parameters

### Multi-Attribute Utility Instrument (MAUI) parameters
We also need to provide information specific to the MAUI used to collect the data from which health utility will be derived. The minimum allowable health utility weight for this instrument can be declared using the `utl_min_val_1L_dbl` argument.

```{r}
maui_params_ls <- make_maui_params_ls(maui_domains_pfcs_1L_chr = "vD",
                                      maui_itm_short_nms_chr = c("Household tasks", "Getting around","Morbility","Self care","Enjoy close rels","Family rels", "Community involvement","Despair","Worry", "Sad", "Agitated","Energy level","Control", "Coping","Frequency of pain", "Degree of pain","Pain interference","Vision", "Hearing","Communication"),
                                      maui_scoring_fn = youthvars::add_adol6d_scores,
                                      short_and_long_nm = c("AQoL-6D",
                                                            "Assessment of Quality of Life - Six Dimension"),
                                      utl_min_val_1L_dbl = 0.03)
```


## Analysis parameters
We can now create a list of input parameters using the `make_input_params` function, principally using the objects created in the preceding steps. We can force the use of specified model types or covariates (rather than allowing the analysis algorithm to select these based on automated criteria) by using the `prefd_mdl_types_chr` and `prefd_covars_chr` arguments. We can also specify how we want to adapt our primary analysis input parameters for secondary analyses (e.g. that use different combinations of candidate predictors and covariates) using the `scndry_anlys_params_ls ` argument in conjunction with the `make_scndry_anlys_params` function. If you have created an empty dataverse dataset to post results to, assign the DOI for your dataset and the name of the dataverse containing it to the `dv_ds_nm_and_url_chr` argument (these values will be unique to you and different to those below, which will not work for you). If you do not wish to post results to a dataverse dataset, set `dv_ds_nm_and_url_chr = NULL`. The `make_input_params` function performs a number of checks relating to the naming conventions used in some variables of interest and where necessary (for consistency with reporting templates) modifies the names of these variables.

```{r eval = F }
input_params_ls <- make_input_params(ds_tb,
                                     ds_descvs_ls = ds_descvs_ls,
                                     dv_ds_nm_and_url_chr = c("fakes", # Dataverse containing dataset
                                                              "https://doi.org/10.7910/DVN/D74QMP"), # DOI
                                     header_yaml_args_ls = header_yaml_args_ls,
                                     maui_params_ls = maui_params_ls,
                                     output_format_ls = output_format_ls,
                                     predictors_lup = predictors_lup,
                                     prefd_covars_chr = "SOFAS",
                                     prefd_mdl_types_chr = c("OLS_CLL","GLM_GSN_LOG"),
                                     scndry_anlys_params_ls = make_scndry_anlys_params(candidate_predrs_chr = c("SOFAS"),
                                                   prefd_covars_chr = NA_character_))

```

# Run analysis
All analytic steps for our primary and secondary analyses are executed with the following function call. Note, this step can involve long execution times (potentially over an hour, depending on the input parameters).
```{r eval=FALSE}
write_analyses(input_params_ls)
```

# Report results
We now generate the catalogues (one for the primary and one for each secondary analysis) that report the models created in the preceding step. The `write_mdl_smry_rprt` function also creates shareable versions (i.e. with copies of source dataset replaced by synthetic data) of the models. This step can take several minutes to execute.

```{r echo = TRUE, eval=FALSE }
input_params_ls <- write_mdl_smry_rprt(input_params_ls,
                                       use_shareable_mdls_1L_lgl = T)
```

# Share results
We can now share the results created in the previous step by posting them to the online repository, the details of which we supplied earlier. 

```{r echo = TRUE, eval=FALSE }
write_study_outp_ds(input_params_ls)
```

The `write_study_outp_ds` function writes its outputs to a draft dataset - which you can review by logging into your dataverse account, before clicking on the option to publish the dataset. The stucture of your draft dataset should be similar to those produced by this vignette, which are viewable at: https://doi.org/10.7910/DVN/D74QMP

# Create manuscript
The first step to creating a manuscript that provides the scientific summary of the study is to add some metadata about the study itself to our parameters list, using the `make_study_descs_ls` function. If you want the manuscript to refer to some of the candidate predictors by different names than that provided in the `short_name_chr` column of the `predictors_lup` object defined earlier, you can create a table of name correspondences and pass it to the `var_nm_change_lup` argument.

```{r eval = F}
input_params_ls <- make_study_descs_ls(input_params_ls = input_params_ls,
                                       background_1L_chr = "Our study is entirely fictional and has been created to illustrate TTU package functionality.",
                                       coi_1L_chr = "None declared.",
                                       conclusion_1L_chr = "If this study was real, the results would be interesting.",
                                       ethics_1L_chr = "The study was reviewed and granted approval by Awesome University's Human Research Ethics Committee (1111111.1).",
                                       funding_1L_chr = "The study was funded by Generous Benefactor.",
                                       sample_desc_1L_chr = "The study sample is fake data that pretends to be young people aged 12 to 25 years who attended Australian primary care services for mental health related needs between November 2019 to August 2020.",
                                       time_btwn_bl_and_fup_1L_chr = "three months",
                                       var_nm_change_lup = tibble::tibble(old_nms_chr = c("PHQ9","GAD7"),
                                                                          new_nms_chr = c("PHQ-9",
                                                                                          "GAD-7")))
```

We can now pass the updated input parameters list to the `write_manuscript` function. This will produce an initial draft of a scientific manuscript (detailed methods and results, a basic introduction and no discussion), by first downloading an extension to the TTU package (https://github.com/ready4-dev/ttu_lng_ss).  If (AND ONLY IF), you are sure that you will never develop the manuscript further with a view to future publication, you can add this automatically generated draft to the study dataset by adding `write_to_dv_1L_lgl = T` to the function call (as below). 

```{r eval = F}
results_ls <- write_manuscript(input_params_ls = input_params_ls,
                               write_to_dv_1L_lgl = T)
```

The document exported to the dataset by the preceding call is accessible at: https://dataverse.harvard.edu/api/access/datafile/4935036 (it is in ".docx" format as we specified "Word" as our desired output format (see earlier in the workflow)). You can override the output format specified in the input parameter list object by using the `output_type_1L_chr` argument. Note, that this time around we are using the `results_ls` object as the main input as this object was automatically saved (to the directory path stored in `results_ls$path_params_ls$paths_ls$output_data_dir_1L_chr`) by the preceding call.

```{r eval = F}
write_manuscript(results_ls = results_ls,
                 write_to_dv_1L_lgl = T,
                 output_type_1L_chr = "PDF")
```

The PDF version is available at: https://dataverse.harvard.edu/api/access/datafile/4935022 .When running the `write_manuscript` function, you can discover the directory into which the manuscript has been saved locally on your machine by printing the object `results_ls$path_params_ls$paths_ls$reports_dir_1L_chr`.

If you wish to develop the manuscript into something you will submit for publication, there are two options for doing so:

- If your planned submission format is PDF or LaTeX, edit the markdown files (printing object `results_ls$path_params_ls$paths_ls$path_to_ms_mkdn_1L_dir` will tell you where these have been saved) and run `write_manuscript(results_ls = results_ls, output_type_1L_chr = "PDF")`. Both PDF and TeX files are saved as a result of this command.

- If your planned submission format is Word, you can just edit the word document rendered by the `write_manuscript` function. This is relatively straightforward, though  unlike the PDF output, you will need to manually edit some of the section numbering and table formatting / headings. 

# Purge dataset copies
As a final step, it is important to delete all local copies (but not the original version) of the study dataset that were produced during the analysis.

```{r echo = TRUE, eval=FALSE }
write_to_delete_ds_copies(path_params_ls$paths_ls)
```

\elandscape
<!---BLOCK_LANDSCAPE_STOP--->
